import json
import numpy as np
from numpy import *
import scipy.sparse as sps
import argparse
from collections import defaultdict
import pandas as pd

#takes a few minutes to run
def create_links_matrix(graph, length):
    #create matrix for links
    
    #create dictionary for json list
    d = defaultdict(list)
    for n1 in graph:
        if (n1 not in d):
            check = 0
            for n2 in graph[n1]:
                d[n1].append(n2)
                check = 1
            if check == 0:
                d[n1].append(None)

    #add missing urls to the dictionary
    count = 0
    while (count < length):
        if (str(count) not in d):
            d[count].append(None)
        count = count + 1
    #print(d.items())

    #loop through dictionary and create matrix
    c = 0
    rows, cols = length, length
    matr = sps.lil_matrix((rows, cols))
    value = 1/(length)
    while (c < length):
        #print(c)
        ls = d[str(c)]
        la = set(ls)
        ls = list(la)
        #print(ls)
        if len(ls) == 0 or ls[0] is None:
            matr[:,c] = value
            #print("Changed 0s")
            #print(matr.todense())
        else:
            #print("adding all")
            for j in ls:
                matr[j,c] = 1/len(ls)
            #print(matr.todense())
        matr[c, c] = 0
        c = c + 1
    
    #print("RETURNING")
    return matr

def pagerank(M, length, eps=0.05):
    matrix = M.asformat("csr")
    value = 1/(length)
    #first iteration
    #print("Intializing array")
    v = np.full((6667), value)
    #print("Dot product")
    #print("Iteration 1")
    page_scores_first = np.dot(matrix.toarray(), v)
    check = 1
    num_iter = 0
    while check == 1:
        #print("Dot product")
        page_scores_temp = np.dot(matrix.toarray(), page_scores_first)
        #check for epsilon
        c = 0
        while (c < length):
            #print("C: ", c)
            if np.abs(page_scores_temp[c] - page_scores_first[c]) / page_scores_temp[c] >= eps:
                check = 0
            c+= 1
        #print(page_scores_temp)
        page_scores_return = page_scores_first
        page_scores_first = page_scores_temp
        num_iter += 1

    return page_scores_return, num_iter


if __name__ == '__main__':
    #eligible arguments to read in
    arg_parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    arg_parser.add_argument('--E', help='weight parameter',
                type=float, default=0.15)
                
    arg_parser.add_argument('--epsilon', help='convergence detection parameter',
                type=float, default=0.05)
                
    arg_parser.add_argument('--adj_list', help='path to the adjacency list file (JSON)',
                type=str, default='/homes/cs473/project2/adj_list.json')
                
    arg_parser.add_argument('--url_dict', help='path to URL to ID mapping file (JSON)',
                type=str, default='/homes/cs473/project2/url_dict.json')

    arg_parser.add_argument('--url_dict_reverse', help='path to ID to URL mapping file (JSON)',
                type=str, default='/homes/cs473/project2/url_dict_reverse.json')

    arg_parser.add_argument('--output_path', help='path to the HITS output file (will be generated by this script)',
                type=str, default='./page_rank_scores.json')
                            
    arg_parser.add_argument('--query_url', help='URL for which pagerank scores need to be output',
                type=str, default='none')

    arg_parser.add_argument('--k', help='number of pageranks (top-k) for which the scores should be printed',
                type=int, default=5)
                            
    args = arg_parser.parse_args()

    #read in all the lists
    adj_list = json.load(open(args.adj_list, 'r'))
    url_dict = json.load(open(args.url_dict, 'r'))
    url_dict_reverse = json.load(open(args.url_dict_reverse, 'r'))
    length = len(url_dict)

    M = create_links_matrix(adj_list, length)
    page_scores, iterations = pagerank(M, length, eps=args.epsilon)
    #print(page_scores)
    #print(iterations)

    #convert page_scores to dictionary
    length = len(page_scores)
    page_dict = {}
    i = 0
    while i < length:
        page_dict[i] = page_scores[i]
        i += 1

    #output to a json file
    res_dict = {}
    res_dict['num_iters'] = iterations
    res_dict['page_rank'] = page_dict
    open(args.output_path, 'w').write(json.dumps(res_dict))

    #print(page_dict)
    if args.query_url is not 'none':
        print('Query URL: ' + args.query_url)
        if args.query_url in url_dict:
            strNum = url_dict[args.query_url]
            index = int(strNum)
            print('Page Rank Score: ', page_dict[index])
        else:
            print('Page Rank Score: 0')

    if args.k > 0:
        #Print top-k hub scores
        s = [(k, page_dict[k]) for k in sorted(page_dict, key=page_dict.get, reverse=True)]
        print('\nTop-' + str(args.k) + ' page-ranks:')
        check = 1
        increment = 0
        for k, v in s:
            if increment == int(args.k):
                break
            print(url_dict_reverse[str(k)]," ", v)
            increment = increment + 1
